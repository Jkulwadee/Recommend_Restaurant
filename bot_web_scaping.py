# -*- coding: utf-8 -*-
"""bot_web_scaping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17y0mDIrOzGfdthooJ6rR_eepjJNspR-l
"""

pip install gradio

# Commented out IPython magic to ensure Python compatibility.
# # Set up for running selenium in Google Colab
# ## You don't need to run this code if you do it in Jupyter notebook, or other local Python setting
# %%shell
# sudo apt -y update
# sudo apt install -y wget curl unzip
# wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb
# dpkg -i libu2f-udev_1.1.4-1_all.deb
# wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
# dpkg -i google-chrome-stable_current_amd64.deb
# CHROME_DRIVER_VERSION=`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`
# wget -N https://chromedriver.storage.googleapis.com/$CHROME_DRIVER_VERSION/chromedriver_linux64.zip -P /tmp/
# unzip -o /tmp/chromedriver_linux64.zip -d /tmp/
# chmod +x /tmp/chromedriver
# mv /tmp/chromedriver /usr/local/bin/chromedriver
# pip install selenium

# Install the library using pip
!pip install sentence-transformers
! pip install sent2vec

from sentence_transformers import SentenceTransformer, util

# Load the BERT model. Various models trained on Natural Language Inference (NLI) https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/nli-models.md and
# Semantic Textual Similarity are available https://github.com/UKPLab/sentence-transformers/blob/master/docs/pretrained-models/sts-models.md

#model = SentenceTransformer('bert-base-nli-mean-tokens')
model = SentenceTransformer('sentence-transformers/paraphrase-MiniLM-L6-v2')

import numpy as np
from sent2vec.vectorizer import Vectorizer
from scipy import spatial
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import re
import requests
from bs4 import BeautifulSoup

def scrape_restaurants(url):
    restaurant_data = []

    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content of the page using BeautifulSoup
        mysoup = BeautifulSoup(response.text, 'html.parser')

        # Find the elements containing the restaurant names
        restaurant_elements = mysoup.find_all('div', style='flex:1;display:flex;flex-direction:column')


        for restaurant in restaurant_elements:

            prices = []
            # Loop through the restaurant elements and extract the data
            name = restaurant.find('h2', class_="HotelItemTitleText-sc-9lfuiu-7 cqmoRq").text.strip()
            score = restaurant.find('span', class_='HotelItemCurrentScore-sc-9lfuiu-12 jcSeg').text.strip()
            location = restaurant.find('span', class_='DistanceInfo-v5q8p3-4 kHfPCe').text.strip()
            price = restaurant.find('span', class_="AveragePrice-sc-9lfuiu-21 eMXsrE").text.strip()

            prices.append(price)
            numeric_prices = [int(re.search(r'\d+', price).group()) for price in prices]

            for i in range(len(prices)):
              restaurant_data.append((name, score, location, numeric_prices[i]))
    else:
        print("Failed to retrieve the webpage. Status code:", response.status_code)

    return restaurant_data

# Example usage:
url = "https://th.trip.com/toplist/tripbest/bangkok-thai-restaurants-100900004994?allianceid=14895&sid=1621248&ppcid=ckid-_adid-_akid-_adgid-&utm_source=google&utm_medium=cpc&utm_campaign=20267085571&gclid=CjwKCAjwgsqoBhBNEiwAwe5w07LzucibcI1j5AP8_2ZWSiZW-8Gjg38BKwbb52tmqPZt_jyg77PU3BoCg6wQAvD_BwE&gclsrc=aw.ds&fbclid=IwAR3az6S-9cNhia8ZYkFDttVCY_XljJB1fDwQpQ2K3bvnhkKXt5yQ58HmKvA"
restaurant_list = scrape_restaurants(url)
for restaurant in restaurant_list:
    print(restaurant)

import pandas as pd

# df contains 6 columns -> name, contact, open time, location, webpage, map
restaurant_df = pd.DataFrame(restaurant_list, columns = ["Name", "Score",  "Location",  "Price"], index = [i+1 for i in range(len(restaurant_list))])
restaurant_df

restaurant_df.sort_values(by=['Price'])

def get_similarity(input, corpus):
  query_vec = model.encode(input, convert_to_tensor=True, normalize_embeddings=True)
  corpus_vec = model.encode(corpus, convert_to_tensor=True, normalize_embeddings=True)

  cosine_scores = util.cos_sim(query_vec, corpus_vec)
  entity_np = np.array(cosine_scores)
  score = np.max(entity_np)
  return score

def greeting(question):
    # Define the corpus within the function
    corpus = ["สวัสดี", "ดีจ้า", "หวัดดี", "โหล", "โหลๆๆ"]
    stop_word = ["ไกล", "ใกล้","ไกลจ้า", "ใกล้จ้า","ช่วย", "ช่วยหน่อย", "ช่วยด้วย","ต่ำกว่า1000","ต่ำกว่า 1000","มี", "มีค่ะ","ช่วยค่ะ","ต้องการค่ะ", "ต้องการทราบ","ทราบ"]
    similarity = get_similarity(corpus,question)
    if question in stop_word:
      similarity*= 0
      return similarity
    else:
      return similarity

def question_1(question):
    # Define the corpus within the function
    corpus = ["มี", "ช่วย", "ช่วยหน่อย", "ช่วยด้วย", "มีค่ะ","ช่วยค่ะ"]
    stop_word = ["ไกล", "ใกล้","ไกลจ้า", "ใกล้จ้า","ต่ำกว่า1000","ต่ำกว่า 1000","ต้องการ", "ต้องการค่ะ", "ต้องการทราบ","ทราบ"]
    similarity = get_similarity(corpus,question)
    if question in stop_word:
      similarity*= 0
      return similarity
    else:
      return similarity

def question_2(question):
    # Define the corpus within the function
    corpus = ["ใกล้", "ไกล", "ใกล้จ้า","ไกลจ้า"]
    stop_word = ["มี", "ช่วย", "ช่วยหน่อย", "ช่วยด้วย","ต่ำกว่า1000","ต่ำกว่า 1000","ต้องการ", "ต้องการค่ะ", "ต้องการทราบ","ทราบ"]
    similarity = get_similarity(corpus,question)
    if question in stop_word:
      similarity*= 0
      return similarity
    else:
      return similarity

def question_3(question):
    # Define the corpus within the function
    # corpus = ["ใกล้", "ไกล", "ใกล้จ้า","ไกลจ้า"]
    corpus = ["ต้องการ", "ต้องการค่ะ", "ต้องการทราบ","ทราบ"]
    stop_word =  ["มี", "ช่วย", "ช่วยหน่อย", "ช่วยด้วย","ต่ำกว่า1000","ต่ำกว่า 1000"]
    similarity = get_similarity(corpus,question)
    if question in stop_word:
      similarity*= 0
      return similarity
    else:
      return similarity

def question_4(question):
    # Define the corpus within the function
    corpus = ["ต่ำกว่า 1,000 บาท", "ต่ำกว่า", "ถูกกว่า", "1000", "หนึ่งพัน","น้อยกว่า 1000","1,000-3,000 บาท ", "2,000", "1000-3000", "2000","2000-3000","1000-2000","1,000-3,000""3,000 บาทขึ้นไป","3000", "ขึ้นไป","แพง","มากสุด"]
    stop_word = ["ต้องการ", "มี", "ช่วย", "ช่วยหน่อย", "ช่วยด้วย","ไกล", "ใกล้","ไกลจ้า", "ใกล้จ้า"]
    similarity = get_similarity(corpus,question)
    if question in stop_word:
      similarity*= 0
      return similarity
    else:
      return similarity

# selecting rows based on condition
data_1 = restaurant_df.loc[restaurant_df['Price'] <1000]
data_1

min_price = 1000
max_price = 3000
data_2 = restaurant_df[(restaurant_df['Price'] >= min_price) & (restaurant_df['Price'] < max_price)]
data_2

data_3 = restaurant_df.loc[restaurant_df['Price'] >=3000]
data_3

import pandas as pd

def filter_data(data):
    result = []
    # Check if there are rows in the filtered DataFrame
    if not data.empty:
        # Loop through and append rows to the result list
        for index, row in data.iterrows():
            result.append(f"Name: {row['Name']}")
            result.append(f"Score: {row['Score']}")
            result.append(f"Location: {row['Location']}")
            result.append(f"Price: {row['Price']} ")
            result.append(' ')
        return '\n'.join(result)  # Join the list elements with newline characters

    else:
        return "No data found matching the criteria."

def compound(question):
    ques_list = []

    ques_greet = greeting(question)
    ques_1 = question_1(question)
    ques_2 = question_2(question)
    ques_3 = question_3(question)
    ques_4 = question_4(question)

    ques_list.append(ques_greet)
    ques_list.append(ques_1)
    ques_list.append(ques_2)
    ques_list.append(ques_3)
    ques_list.append(ques_4)

    index_ques = np.argmax(ques_list)
    if index_ques == 0:
      return "สวัสดีค่ะ เราเป็นบอทให้คำแนะนำเกี่ยวกับร้านอาหารในกรุงเทพ มีอะไรต้องการให้ช่วยไหมคะ"

    elif index_ques == 1:
      return "ลูกค้าอยากได้ร้านอาหารใกล้หรือไกลจากตัวเมืองกรุงเทพคะ"

    elif index_ques == 2:
      return "ลูกค้าต้องการทราบคะแนนรีวิวของทางร้านไหมคะ"

    elif index_ques == 3:
      return "ลูกค้าอยากได้ราคาประมาณไหนแจ้งได้เลยนะคะ(พิมพ์ตัวเลขได้เลยค่ะ)"

    elif index_ques == 4:
      if question in ["ต่ำกว่า 1,000 บาท", "ต่ำกว่า", "ถูกกว่า", "1000", "หนึ่งพัน","น้อยกว่า 1000","1,000"]:
        data_cheap = filter_data(data_1)
        return "จากการสอบถามลูกค้าเบื้องต้นแล้ว เราได้เลือกร้านที่ราคาไม่เกิน 1,000 บาทมาดังนี้เลยค่ะ\n\n"+data_cheap+"\nหวังว่าจะมีร้านที่ถูกใจลูกค้าและลูกค้าประทับใจนะคะ"

      elif question in ["1,000-3,000 บาท ", "2,000", "1000-3000", "2000","2000-3000","1000-2000","1,000-3,000"]:
        data_normal = filter_data(data_2)
        return "จากการสอบถามลูกค้าเบื้องต้นแล้ว เราได้เลือกร้านที่มีราคาอยู่ในช่วง 1,000-3,000 บาทมาดังนี้เลยค่ะ\n\n"+data_normal+"\nหวังว่าจะมีร้านที่ถูกใจลูกค้าและลูกค้าประทับใจนะคะ"

      elif question in  ["3,000 บาทขึ้นไป","3000", "ขึ้นไป","แพง","มากสุด","4000","5000"]:
        data_expensive = filter_data(data_3)
        return "จากการสอบถามลูกค้าเบื้องต้นแล้ว เราได้เลือกร้านที่มีราคา 3,000 บาทขึ้นไปมาดังนี้เลยค่ะ\n\n"+data_expensive+"\nหวังว่าจะมีร้านที่ถูกใจลูกค้าและลูกค้าประทับใจนะคะ"

!pip install pyngrok
!pip install flask_ngrok
!pip install line-bot-sdk

from flask_ngrok import run_with_ngrok
from flask import Flask, request

# 載入 LINE Message API 相關函式庫
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import MessageEvent, TextMessage, TextSendMessage

import json

app = Flask(__name__)

@app.route("/", methods=['POST'])
def linebot():
    body = request.get_data(as_text=True)
    try:
        json_data = json.loads(body)
        access_token = '7cd/jsxlpfWyKn8Xj0EOgq1DwXqigU0AA6yFYEl+QMhJ8n7NyPY17NBlb0IkeeV6BpXVanE7QeFyeGBWEICjREytNQAww4/uOPsUxQ/VQ+AxXI7TSJ3Q74nOOAORz29QHSw3nEKd41ISlIzdZlu0XAdB04t89/1O/w1cDnyilFU='
        secret = 'd34d0d79a095420d5db0cb4f3b1d8c25'
        line_bot_api = LineBotApi(access_token)
        handler = WebhookHandler(secret)
        signature = request.headers['X-Line-Signature']
        handler.handle(body, signature)
        msg = json_data['events'][0]['message']['text']
        tk = json_data['events'][0]['replyToken']
        msg_reply = compound(msg)
        line_bot_api.reply_message(tk,TextSendMessage(text=msg_reply,quick_reply=msg_reply))
        print(msg, tk)
    except:
        print(body)                                          # 如果發生錯誤，印出收到的內容
    return 'OK'                 # 驗證 Webhook 使用，不能省略
if __name__ == "__main__":
  run_with_ngrok(app)           # 串連 ngrok 服務
  app.run()